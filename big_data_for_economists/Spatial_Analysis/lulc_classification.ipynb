{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction\n",
                "In this chapter we will classify data from the Sentinel-2 satellite  using a supervised classification approach which incorporates the training data represented as a vector (shapefile). Specifically, we will be using [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes). Naive Bayes predicts the probabilities that a data point belongs to a particular class and the class with the highest probability is considered as the most likely class. The way they get these probabilities is by using Bayesâ€™ Theorem, which describes the probability of a feature, based on prior knowledge of conditions that might be related to that feature. Naive Bayes is quite fast when compared to some other machine learning approaches (e.g., SVM can be quite computationally intensive). This isn't to say that it is the best per se; rather it is a great first step into the world of machine learning for classification and regression.\n",
                "\n",
                "## Preparing the dataset\n",
                "#### Opening the images\n",
                "Our first step is to import the relevant packages. Of note are a couple new ones, namely rasterio and shapely. These are excellent libraries that simplify working with rasters and vectors (respectively). They wrap around GDAL so you don't have to do it the hard way. They also provide some very useful helper functions, like mask and mapping, which we explicitly import below."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import rasterio\n",
                "from rasterio.mask import mask\n",
                "import geopandas as gpd\n",
                "import numpy as np\n",
                "from shapely.geometry import mapping"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we need to collect all the Sentinal-2 bands because they come as individual images one per band. Ultimately, we're going to rewrite them into a multi-band (8 bands) geotiff for later use in regression."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import os # we need os to do some basic file operations\n",
                "\n",
                "data_dir = \"../data/lulc_classification_example/\"\n",
                "\n",
                "sentinal_fp = os.path.join(data_dir, \"sentinel-2/\")\n",
                "\n",
                "# If this isn't working it's probably because you have the file saved somewhere else than where sentinal_fp points. \n",
                "# Examine the absolute path to investigate.\n",
                "\n",
                "\n",
                "a = os.path.abspath(sentinal_fp)\n",
                "# find every file in the sentinal_fp directory\n",
                "sentinal_band_paths = [os.path.join(sentinal_fp, f) for f in os.listdir(sentinal_fp) if os.path.isfile(os.path.join(sentinal_fp, f))]\n",
                "sentinal_band_paths.sort()\n",
                "sentinal_band_paths"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below we will create a rasterio dataset object containing all bands in order to use the mask() function and extract pixel values using geospatial polygons.\n",
                "\n",
                "We'll do this by creating a new raster dataset and saving it for future uses."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# create a products directory within the data dir which won't be uploaded to Github\n",
                "workspace_dir = '../../textbook_workspace'\n",
                "img_dir = os.path.join(workspace_dir, 'generated_lulc')\n",
                "\n",
                "# check to see if the dir it exists, if not, create it\n",
                "if not os.path.exists(img_dir):\n",
                "    os.makedirs(img_dir)\n",
                "\n",
                "# filepath for image we're writing out\n",
                "img_fp = os.path.join(img_dir, 'sentinel_bands.tif')\n",
                "\n",
                "# Read metadata of first file and assume all other bands are the same\n",
                "with rasterio.open(sentinal_band_paths[0]) as src0:\n",
                "    meta = src0.meta\n",
                "\n",
                "# Update metadata to reflect the number of layers\n",
                "meta.update(count = len(sentinal_band_paths))\n",
                "\n",
                "# Read each layer and write it to stack\n",
                "with rasterio.open(img_fp, 'w', **meta) as dst:\n",
                "    for id, layer in enumerate(sentinal_band_paths, start=1):\n",
                "        with rasterio.open(layer) as src1:\n",
                "            dst.write_band(id, src1.read(1))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Okay we've successfully written it out now let's open it back up and make sure it meets our expectations:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "full_dataset = rasterio.open(img_fp)\n",
                "img_rows, img_cols = full_dataset.shape\n",
                "img_bands = full_dataset.count\n",
                "print(full_dataset.shape) # dimensions\n",
                "print(full_dataset.count) # bands"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise:\n",
                "\n",
                "Let's clip the image and take a look at it. In the starter code below, we use the .read() method to read three different bands into a single image that we will plot. We also use numpy slice notation to clip out a smaller part of the array.\n",
                "\n",
                "HOWEVER, the image won't look right on its own. You can kind of tell there is color, but it doesn't look like it should. That's because we're reading in the wrong bands (our eyes are expecting Red, Green and Blue). \n",
                "\n",
                "As a class race, try out different combinations of bands. When you think you've got it, raise your hand. When someone gets it, or after 2 minutes, whoever has the best image wins! Hint: if you want to nail this, just look at the sentinel documentation https://gprivate.com/62co2."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import matplotlib.pyplot as plt\n",
                "from rasterio.plot import show\n",
                "\n",
                "\n",
                "band_1 = 1\n",
                "band_2 = 2\n",
                "band_3 = 3\n",
                "\n",
                "clipped_img = full_dataset.read([band_1, band_2, band_3])[:, 150:600, 250:1400]\n",
                "print(clipped_img.shape)\n",
                "fig, ax = plt.subplots(figsize=(10,7))\n",
                "show(clipped_img[:, :, :], ax=ax, transform=full_dataset.transform) # add the transform arg to get it in lat long coords"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Okay looks good! Our raster dataset is ready!\n",
                "\n",
                "### Now our goal is to get the pixels from the raster as outlined in each shapefile. \n",
                "\n",
                "Our training data, the shapefile we've worked with, contains one main field we care about:\n",
                "+ a Classname field (String datatype)\n",
                "\n",
                "Combined with the innate location information of polygons in a Shapefile, we have all that we need to use for pairing labels with the information in our raster.\n",
                "\n",
                "However, in order to pair up our vector data with our raster pixels, we will need a way of co-aligning the datasets in space. \n",
                "\n",
                "We'll do this using the rasterio mask function which takes in a dataset and a polygon and then outputs a numpy array with the pixels in the polygon.\n",
                "\n",
                "Let's run through an example:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "full_dataset.crs"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Open up our shapefile and check its crs"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "shapefile = gpd.read_file(os.path.join(data_dir, 'rcr', 'rcr_landcover.shp'))\n",
                "shapefile.crs"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Remember the projections don't match! Let's use some geopandas magic to reproject all our shapefiles to lat, long."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "shapefile = shapefile.to_crs({'init': 'epsg:4326'})"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "shapefile.crs"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "len(shapefile)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we want to extract the geometry of each feature in the shapefile in GeoJSON format:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# this generates a list of shapely geometries\n",
                "geoms = shapefile.geometry.values \n",
                "\n",
                "# let's grab a single shapely geometry to check\n",
                "geometry = geoms[0] \n",
                "print(type(geometry))\n",
                "print(geometry)\n",
                "\n",
                "# transform to GeoJSON format\n",
                "from shapely.geometry import mapping\n",
                "feature = [mapping(geometry)] # can also do this using polygon.__geo_interface__\n",
                "print(type(feature))\n",
                "print(feature)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's extract the raster values values within the polygon using the rasterio [mask() function](https://rasterio.readthedocs.io/en/latest/api/rasterio.mask.html)"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "out_image, out_transform = mask(full_dataset, feature, crop=True)\n",
                "out_image.shape"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Okay those looks like the right dimensions for our training data. 8 bands and 6x8 pixels seems reasonable given our earlier explorations.\n",
                "\n",
                "We'll be doing a lot of memory intensive work so let's clean up and close this dataset."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "full_dataset.close()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Building the Training Data for `scikit-learn`\n",
                "\n",
                "Now let's do it for all features in the shapefile and create an array `X` that has all the pixels and an array `y` that has all the training labels."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "X = np.array([], dtype=np.int8).reshape(0,8) # pixels for training\n",
                "y = np.array([], dtype=np.string_) # labels for training\n",
                "\n",
                "# extract the raster values within the polygon \n",
                "with rasterio.open(img_fp) as src:\n",
                "    band_count = src.count\n",
                "    for index, geom in enumerate(geoms):\n",
                "        feature = [mapping(geom)]\n",
                "\n",
                "        # the mask function returns an array of the raster pixels within this feature\n",
                "        out_image, out_transform = mask(src, feature, crop=True) \n",
                "        \n",
                "        # eliminate all the pixels with 0 values for all 8 bands - AKA not actually part of the shapefile\n",
                "        out_image_trimmed = out_image[:,~np.all(out_image == 0, axis=0)]\n",
                "        \n",
                "        # eliminate all the pixels with 255 values for all 8 bands - AKA not actually part of the shapefile\n",
                "        out_image_trimmed = out_image_trimmed[:,~np.all(out_image_trimmed == 255, axis=0)]\n",
                "        \n",
                "        # reshape the array to [pixel count, bands]\n",
                "        out_image_reshaped = out_image_trimmed.reshape(-1, band_count)\n",
                "        \n",
                "        # append the labels to the y array\n",
                "        y = np.append(y,[shapefile[\"Classname\"][index]] * out_image_reshaped.shape[0]) \n",
                "        \n",
                "        # stack the pizels onto the pixel array\n",
                "        X = np.vstack((X, out_image_reshaped))        "
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Pairing Y with X\n",
                "Now that we have the image we want to classify (our X feature inputs), and the land cover labels (our y labeled data), let's check to make sure they match in size so we can feed them to Naive Bayes:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# What are our classification labels?\n",
                "labels = np.unique(shapefile[\"Classname\"])\n",
                "print('The training data include {n} classes: {classes}\\n'.format(n=labels.size, \n",
                "                                                                classes=labels))\n",
                "\n",
                "# We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels\n",
                "print('Our X matrix is sized: {sz}'.format(sz=X.shape))\n",
                "print('Our y array is sized: {sz}'.format(sz=y.shape))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It all looks good! Let's explore the spectral signatures of each class now to make sure they're actually separable since all we're going by in this classification is pixel values."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "scrolled": true
            },
            "source": [
                "fig, ax = plt.subplots(1,3, figsize=[20,8])\n",
                "\n",
                "# numbers 1-8\n",
                "band_count = np.arange(1,9)\n",
                "\n",
                "classes = np.unique(y)\n",
                "for class_type in classes:\n",
                "    band_intensity = np.mean(X[y==class_type, :], axis=0)\n",
                "    ax[0].plot(band_count, band_intensity, label=class_type)\n",
                "    ax[1].plot(band_count, band_intensity, label=class_type)\n",
                "    ax[2].plot(band_count, band_intensity, label=class_type)\n",
                "# plot them as lines\n",
                "\n",
                "# Add some axis labels\n",
                "ax[0].set_xlabel('Band #')\n",
                "ax[0].set_ylabel('Reflectance Value')\n",
                "ax[1].set_ylabel('Reflectance Value')\n",
                "ax[1].set_xlabel('Band #')\n",
                "ax[2].set_ylabel('Reflectance Value')\n",
                "ax[2].set_xlabel('Band #')\n",
                "#ax[0].set_ylim(32,38)\n",
                "ax[1].set_ylim(32,38)\n",
                "ax[2].set_ylim(70,140)\n",
                "#ax.set\n",
                "ax[1].legend(loc=\"upper right\")\n",
                "# Add a title\n",
                "ax[0].set_title('Band Intensities Full Overview')\n",
                "ax[1].set_title('Band Intensities Lower Ref Subset')\n",
                "ax[2].set_title('Band Intensities Higher Ref Subset')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "They look okay but emergent wetland and subtital haline look quite similar! They're going to be difficult to differentiate.\n",
                "\n",
                "Let's make a quick helper function, this one will convert the class labels into indicies and then assign a dictionary relating the class indices and their names."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def str_class_to_int(class_array):\n",
                "    class_array[class_array == 'Subtidal Haline'] = 0\n",
                "    class_array[class_array == 'WetSand'] = 1\n",
                "    class_array[class_array == 'Emergent Wetland'] = 2\n",
                "    class_array[class_array == 'Sand'] = 3\n",
                "    class_array[class_array == 'Herbaceous'] = 4\n",
                "    class_array[class_array == 'Forested Wetland'] = 5\n",
                "    return(class_array.astype(int))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training the Classifier\n",
                "Now that we have our X matrix of feature inputs (the spectral bands) and our y array (the labels), we can train our model.\n",
                "\n",
                "Visit [this web page to find the usage of GaussianNaiveBayes Classifier](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes) from `scikit-learn`."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from sklearn.naive_bayes import GaussianNB\n",
                "\n",
                "gnb = GaussianNB()\n",
                "gnb.fit(X, y)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It is that simple to train a classifier in `scikit-learn`! The hard part is often validation and interpretation.\n",
                "\n",
                "## Predicting on the image\n",
                "\n",
                "With our Naive Bayes classifier fit, we can now proceed by trying to classify the entire image:\n",
                "\n",
                "We're only going to open the subset of the image we viewed above because otherwise it is computationally too intensive for most users."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from rasterio.plot import show\n",
                "from rasterio.plot import show_hist\n",
                "from rasterio.windows import Window\n",
                "from rasterio.plot import reshape_as_raster, reshape_as_image"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "with rasterio.open(img_fp) as src:\n",
                "    # may need to reduce this image size if your kernel crashes, takes a lot of memory\n",
                "    img = src.read()[:, 150:600, 250:1400]\n",
                "\n",
                "# Take our full image and reshape into long 2d array (nrow * ncol, nband) for classification\n",
                "print(img.shape)\n",
                "reshaped_img = reshape_as_image(img)\n",
                "print(reshaped_img.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we can predict for each pixel in our image:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class_prediction = gnb.predict(reshaped_img.reshape(-1, 8))\n",
                "\n",
                "# Reshape our classification map back into a 2D matrix so we can visualize it\n",
                "class_prediction = class_prediction.reshape(reshaped_img[:, :, 0].shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Because our shapefile came with the labels as strings we want to convert them to a numpy array with ints using the helper function we made earlier."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class_prediction = str_class_to_int(class_prediction)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Let's visualize it!\n",
                "\n",
                "First we'll make a colormap so we can visualize the classes, which are just encoded as integers, in more logical colors. Don't worry too much if this code is confusing! It can be a little clunky to specify colormaps for `matplotlib`."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def color_stretch(image, index):\n",
                "    colors = image[:, :, index].astype(np.float64)\n",
                "    for b in range(colors.shape[2]):\n",
                "        colors[:, :, b] = rasterio.plot.adjust_band(colors[:, :, b])\n",
                "    return colors\n",
                "    \n",
                "# find the highest pixel value in the prediction image\n",
                "n = int(np.max(class_prediction))\n",
                "\n",
                "# next setup a colormap for our map\n",
                "colors = dict((\n",
                "    (0, (48, 156, 214, 255)),   # Blue - Water\n",
                "    (1, (139,69,19, 255)),      # Brown - WetSand\n",
                "    (2, (96, 19, 134, 255)),    # Purple - Emergent Wetland\n",
                "    (3, (244, 164, 96, 255)),   # Tan - Sand\n",
                "    (4, (206, 224, 196, 255)),  # Lime - Herbaceous\n",
                "    (5, (34, 139, 34, 255)),    # Forest Green - Forest \n",
                "))\n",
                "\n",
                "# Put 0 - 255 as float 0 - 1\n",
                "for k in colors:\n",
                "    v = colors[k]\n",
                "    _v = [_v / 255.0 for _v in v]\n",
                "    colors[k] = _v\n",
                "    \n",
                "index_colors = [colors[key] if key in colors else \n",
                "                (255, 255, 255, 0) for key in range(0, n+1)]\n",
                "\n",
                "cmap = plt.matplotlib.colors.ListedColormap(index_colors, 'Classification', n+1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now show the classified map next to the RGB image!"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "fig, axs = plt.subplots(2,1,figsize=(10,7))\n",
                "\n",
                "img_stretched = color_stretch(reshaped_img, [4, 3, 2])\n",
                "axs[0].imshow(img_stretched)\n",
                "\n",
                "axs[1].imshow(class_prediction, cmap=cmap, interpolation='none')\n",
                "\n",
                "fig.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### This looks pretty good!\n",
                "\n",
                "Let's generate a map of Normalized Difference Water Index (NDWI) and NDVI just to compare with out output map.\n",
                "\n",
                "NDWI is similar to NDVI but for identifying water."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "with rasterio.open(img_fp) as src:\n",
                "    green_band = src.read(3)\n",
                "    red_band = src.read(4)\n",
                "    nir_band = src.read(8)\n",
                "    \n",
                "ndwi = (green_band.astype(float) - nir_band.astype(float)) / (green_band.astype(float) + nir_band.astype(float))\n",
                "ndvi = (nir_band.astype(float) - red_band.astype(float)) / (red_band.astype(float) + nir_band.astype(float))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Subset them to our area of interest:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "ndwi = ndwi[150:600, 250:1400]\n",
                "ndvi = ndvi[150:600, 250:1400]"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Display all four maps:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "fig, axs = plt.subplots(2,2,figsize=(15,7))\n",
                "\n",
                "img_stretched = color_stretch(reshaped_img, [3, 2, 1])\n",
                "axs[0,0].imshow(img_stretched)\n",
                "\n",
                "axs[0,1].imshow(class_prediction, cmap=cmap, interpolation='none')\n",
                "\n",
                "nwdi_plot = axs[1,0].imshow(ndwi, cmap=\"RdYlGn\")\n",
                "axs[1,0].set_title(\"NDWI\")\n",
                "fig.colorbar(nwdi_plot, ax=axs[1,0])\n",
                "\n",
                "ndvi_plot = axs[1,1].imshow(ndvi, cmap=\"RdYlGn\")\n",
                "axs[1,1].set_title(\"NDVI\")\n",
                "fig.colorbar(ndvi_plot, ax=axs[1,1])\n",
                "\n",
                "plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Looks pretty good! Areas that are high on the NDWI ratio are generally classified as water and areas high on NDVI are forest and herbaceous. It does seem like the wetland areas (e.g. the bottom right island complex) aren't being picked up so it might be worth experimenting with other algorithms!\n",
                "\n",
                "Let's take a closer look at the Duke Marine Lab and the tip of the Rachel Carson Reserve."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "fig, axs = plt.subplots(1,2,figsize=(15,15))\n",
                "\n",
                "img_stretched = color_stretch(reshaped_img, [3, 2, 1])\n",
                "axs[0].imshow(img_stretched[0:180, 160:350])\n",
                "\n",
                "axs[1].imshow(class_prediction[0:180, 160:350], cmap=cmap, interpolation='none')\n",
                "\n",
                "fig.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This actually doesn't look half bad! Land cover mapping is a complex problem and one where there are many approaches and tools for improving a map.\n",
                "\n",
                "## Testing an Unsupervised Classification Algorithm\n",
                "\n",
                "Let's also try a unsupervised classification algorithm, k-means clustering, in the scikit-learn library ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html))\n",
                "\n",
                "K-means ([wikipedia page](https://en.wikipedia.org/wiki/K-means_clustering)) aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from sklearn.cluster import KMeans\n",
                "\n",
                "bands, rows, cols = img.shape\n",
                "\n",
                "k = 10 # num of clusters\n",
                "\n",
                "kmeans_predictions = KMeans(n_clusters=k, random_state=0).fit(reshaped_img.reshape(-1, 8))\n",
                "\n",
                "kmeans_predictions_2d = kmeans_predictions.labels_.reshape(rows, cols)\n",
                "\n",
                "# Now show the classmap next to the image\n",
                "fig, axs = plt.subplots(1,2,figsize=(15,8))\n",
                "\n",
                "img_stretched = color_stretch(reshaped_img, [3, 2, 1])\n",
                "axs[0].imshow(img_stretched)\n",
                "\n",
                "axs[1].imshow(kmeans_predictions_2d)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Wow this looks like it was better able to distinguish some areas like the wetland and submerged sand than our supervised classification approach! But supervised usually does better with some tuning, luckily there are lots of ways to think about improving our supervised method.\n",
                "\n",
                "Adapted from the wonderful tutorial series by Patrick Gray: https://github.com/patrickcgray\n"
            ]
        }
    ],
    "metadata": [
        {
            "kernelspec": {
                "name": "python3",
                "language": "python",
                "display_name": "Python 3 (ipykernel)"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 4
}